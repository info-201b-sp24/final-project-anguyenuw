---
title: "testingrmd"
author: "Anh-Minh Nguyen"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
install.packages("PlayerRatings")
install.packages("dplyr")
library("PlayerRatings")
library("dplyr")
setwd("C:/Users/mrche/info201/exploratory-analysis-anguyenuw")

#method 1: compare players to their opponents and teammates
# sort by datetime, 
# per match, per map, log each player's score
# then, send this log into EloM.
```{r}

base <- c(50, 40, 32, 26, 22, 18, 14, 10, -10, -14, -18, -22, -26, -32, -40, -50)
base <- c(50, 40, 32, 26, 22, 18, 14, 10, -10, -14, -18, -22, -26, -32, -40, -50)
#TODO - iteratively run the elom algo for each map, using an appropriately sized base vector
base3 <- c(0, 0, 0, 0, 0, 0, 0, 0, -10, -14, -18, -22, -26, -32, -40, -50)
base2 <- c(30,10,-10,-30,0,0,0,0,0)
base4 <- c(30, 10, -10, -30)
# loop through each match
# loop through each map
max_players <- 9
player_vec = numeric(max_players)
score_vec = numeric(max_players)

by_dt <- arrange(f, Datetime)

score_history <- data.frame(matrix(ncol = 1 + 2*max_players, nrow = 0))
#colnames(score_history) <- c("Time", "p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8", "p9", "p10", "p11", "p12", "p13", "p14", "p15", "p16", "score1", "score2", "score3", "score4", "score5", "score6", "score7", "score8", "score9", "score10", "score11", "score12", "score13", "score14", "score15", "score16")
colnames(score_history) <- c("Time", "p1", "p2", "p3", "p4", "p5", "p6", "p7", "p8", "p9", "score1", "score2", "score3", "score4", "score5", "score6", "score7", "score8", "score9")
#colnames(score_history) <- c("Time", "p1", "p2", "p3", "p4", "score1", "score2", "score3", "score4")

# for each match,
match_ids <- unique(by_dt$Match.ID)
#match_ids <- unique(f$Match.ID)[1]
for (match_id in match_ids) {
  match <- by_dt[by_dt$Match.ID == match_id,]
  maps <- unique(match$Map.ID)
# for each map,
  for (map in maps) {
    scores <- match[match$Map.ID == map, ]
# add a vector of the form (Time, p1_id, p2_id, ... p1_score, p2_score, ...)
    player_vec <- numeric(max_players)
    score_vec <- numeric(max_players)
    num_players <- nrow(scores)
    date <- as.numeric(strptime(scores$Datetime[1], format="%Y-%m-%d %H:%M:%S", tz="UTC"))
    for (player_n in 1:max_players) {
  #TODO: should be num_players
      if (player_n <= max_players) {
        player_vec[player_n] <- scores$Username[player_n]
        score_vec[player_n] <- scores$Score[player_n]
        }
      else {
        player_vec[player_n] <- NA
        score_vec[player_n] <- NA
      }
    }

    score_history[nrow(score_history) + 1, 1] <- date
    score_history[nrow(score_history), 2:(1 + max_players)] <- player_vec
    score_history[nrow(score_history), (2 + max_players):(1 + 2*max_players)] <- score_vec
  }
}
osu_ratings_m1 <- elom(score_history, nn = max_players, exact = FALSE, base = base2, history = FALSE)
```

#method 2: compare players to everyone who played the same map, 
#            within a certain timeframe

# sort by datetime
# while there are maps to be processed:
#   take the first map in the dataset.
#   select all scores set on that map in the following 2 weeks.
#     call this subset map_scores.
#     delete map_scores from the dataset.
#   run an EloM iteration on all n scores in map_scores,
#     simulating a player count of n.
#     use a base vector of n values, linearly interpolated from 30 to -30.
```{r}
all_scores <- read.csv("csvfiles/ads_24_d1.csv")
A <- function(x) as.numeric(strptime(x, format="%Y-%m-%d %H:%M:%S", tz="UTC"))
by_datetime <- all_scores
by_datetime$Datetime <- unlist(lapply(all_scores$Datetime, A))
by_datetime <- arrange(by_datetime, Datetime)

#map1 <- by_datetime$Map.ID[1]
#datetime1 <- by_datetime$Datetime[1]
#sample_case <- filter(by_datetime, Map.ID == map1, Datetime < datetime1 + 100000)

# expressed in seconds
# 1209600 seconds = 2 weeks
time_limit <- 1209600

osu_ratings_m2 <- NULL
while (nrow(by_datetime) > 0) {
  # get data on the earliest map in the dataset
  map <- by_datetime$Map.ID[1]
  time <- by_datetime$Datetime[1]
  
  # split dataset into scores on this map + within timeframe, and every other score
  #map_scores <- filter(by_datetime, Map.ID == map, Datetime < time + time_limit)
  map_scores <- by_datetime %>%
    filter(Map.ID == map, 
           Datetime < time + time_limit)
  
  #by_datetime <- filter(by_datetime, (Map.ID != map) | (Datetime >= time + time_limit))
  by_datetime <- by_datetime %>%
    filter((Map.ID != map) |
             (Datetime >= time + time_limit))
  
  # set up inputs to elom
  num_players <- nrow(map_scores)
  
  # skip if only 1 player is found
  if (num_players==1) next
  
  base <- seq(30, -30, length.out=num_players)
  players_and_scores <- data.frame(matrix(ncol = 1 + 2*num_players, nrow = 0))
  player_vec <- map_scores$Username
  score_vec <- map_scores$Score
  
  players_and_scores[1, 1] <- time
  players_and_scores[1, 2:(1 + num_players)] <- player_vec
  players_and_scores[1, (2 + num_players):(1 + 2*num_players)] <- score_vec
  
  # apply elom
  osu_ratings_m2 <- elom(players_and_scores, 
                         nn = num_players,
                         base = base, 
                         status = osu_ratings_m2$ratings,
                         history = FALSE)
}
View(osu_ratings_m2$ratings)
```

#method 3: same as method 2, but more efficient

#phase 1 - collect scores
#  group by map ID, split by groups
#  list score_history stores inputs to elom
#  df time_indices stores associations between Datetime 
#     and entry indices in score_history
#  for each group -
#     while there are scores to be processed:
#       select all scores set on that map in the following 2 weeks.
#         call this subset map_scores.
#         delete map_scores from the group.
#       process into a 1-row dataframe containing timestamp, players, scores
#       add an entry to score_history with timestamp and matrix
#       add a row to time_indices with timestamp and latest score_history index

#phase 2 - sort score vectors, calculate elo
#  sort score history by datetime
#  iteratively send shards through elom in temporal order
#    send in batches to reduce memory costs
```{r}
scores_raw <- read.csv("csvfiles/mp_data/all_mps_0.csv") %>%
          rbind(read.csv("csvfiles/mp_data/all_mps_1.csv")) %>%
          rbind(read.csv("csvfiles/mp_data/all_mps_2.csv")) %>%
          rbind(read.csv("csvfiles/mp_data/all_mps_3.csv")) %>%
          rbind(read.csv("csvfiles/mp_data/all_mps_4.csv")) %>%
          rbind(read.csv("csvfiles/mp_data/all_mps_5.csv")) %>%
          rbind(read.csv("csvfiles/mp_data/all_mps_6.csv"))
scores <- scores_raw %>%
          filter(!is.na(UserID)) %>%
          filter(Score > 10) %>%
          distinct()

# convert Datetimes to Epoch time
A <- function(x) as.numeric(strptime(x, format="%Y-%m-%d %H:%M:%S", tz="UTC"))
scores$Datetime <- unlist(lapply(scores_raw$Datetime, A))
scores <- scores %>% arrange(Datetime)

# expressed in seconds
# 1209600 seconds = 2 weeks
time_limit <- 1209600

# shard data by maps
scores <- scores %>% group_split(MapID)

score_history <- list()
time_indices <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(time_indices) <- c("Datetime", "Index")

# phase 1 - collect scores
for (map_shard in scores) {
  while (nrow(map_shard) > 0) {
    # get data on the earliest score in the shard
    map <- map_shard$Map.ID[1]
    time <- map_shard$Datetime[1]
    
    # split shard into scores within timeframe and scores beyond timeframe
    map_scores <- map_shard %>%
      filter(Datetime < time + time_limit)
    map_shard <- map_shard %>%
      filter(Datetime >= time + time_limit)
    
    # set up input to elom
    n <- nrow(map_scores)
    
    # skip if only 1 player is found
    if (n==1) next
    
    #
    players_and_scores <- data.frame(matrix(ncol = 1 + 2*n, nrow = 0))
    
    players_and_scores[1, 1] <- time
    players_and_scores[1, 2:(1 + n)] <- map_scores$UserID
    players_and_scores[1, (2 + n):(1 + 2*n)] <- map_scores$Score
    
    index <- length(score_history) + 1
    score_history[[index]] <- players_and_scores
    time_indices <- time_indices %>% add_row(Datetime=time, Index=index)
  }
}

# phase 2 - sort score vectors, calc elo in time order
time_indices <- time_indices %>% arrange(Datetime)

osu_ratings_m3 <- NULL
# manually run in batches to keep down memory costs
i_ranges <- list(1:99, 100:999, 1000:4999, 5000:9999, 10000:19999, 20000:34999, 
                 35000:49999, 50000:64999, 65000:79999, 80000:94999, 95000:106975)
i_ranges[[length(i_ranges) + 1]] <- (1000:length(time_indices$Datetime))
for (i in i_ranges[[12]]) {
  index = time_indices$Index[i]
  scorevec <- score_history[[index]]
  n <- (length(scorevec) - 1) / 2
  base <- seq(30, -30, length.out=n)
  # apply elom
  osu_ratings_m3 <- elom(scorevec, 
                           nn = n,
                           base = base, 
                           status = osu_ratings_m3$ratings,
                           history = FALSE)
}

#View(osu_ratings_m3$ratings)
```

# write all user ids found in the dataset
# used by scraper.py to get player info
```{r}
all_player_ids <- scores_raw %>% 
  select(User.ID) %>% 
  unique() %>% 
  unlist(use.names = FALSE)
write.csv(all_player_ids, "csvfiles/all_player_ids.csv", row.names = FALSE)
```

# compile player data with RME ratings
```{r}
player_data <- read.csv("csvfiles/user_data/user_data_0.csv") %>% 
  rbind(read.csv("csvfiles/user_data/user_data_1.csv")) %>%
  rbind(read.csv("csvfiles/user_data/user_data_2.csv")) %>%
  rbind(read.csv("csvfiles/user_data/user_data_3.csv")) %>%
  rbind(read.csv("csvfiles/user_data/user_data_4.csv")) %>%
  rbind(read.csv("csvfiles/user_data/user_data_5.csv")) %>%
  rbind(read.csv("csvfiles/user_data/user_data_6.csv"))

# fill in RME ratings
player_data <- player_data %>%
  left_join(osu_ratings_m3$ratings, by = join_by(Username == Player)) %>%
  mutate(RME = Rating) %>%
  select(UserID, Username, GlobalRank, PP, RME, Location, Games)

# delete NA values
# these may happen because a player was banned between 2023 and now
player_data <- player_data %>%
  filter((!is.na(RME)) & (!is.na(GlobalRank))) 

# append ordinal rankings by RME
player_data <- player_data %>%
  arrange(desc(RME)) %>%
  mutate(RMERanking = 1:length(player_data$UserID), .after=RME)

write.csv(player_data, "csvfiles/user_data/users_with_RME.csv", row.names = FALSE)
```


# verify raw data
```{r}
# find the largest time difference between 2 scores in the dataset 
library(dplyr)
setwd("C:/Users/mrche/info201/exploratory-analysis-anguyenuw")
all <- read.csv("csvfiles/mp_data/all_mps_0.csv") %>% 
  rbind(read.csv("csvfiles/mp_data/all_mps_1.csv")) %>%
  rbind(read.csv("csvfiles/mp_data/all_mps_2.csv")) %>%
  rbind(read.csv("csvfiles/mp_data/all_mps_3.csv")) %>%
  rbind(read.csv("csvfiles/mp_data/all_mps_4.csv")) %>%
  rbind(read.csv("csvfiles/mp_data/all_mps_5.csv")) %>%
  rbind(read.csv("csvfiles/mp_data/all_mps_6.csv"))


A <- function(x) as.numeric(strptime(x, format="%Y-%m-%d %H:%M:%S", tz="UTC"))

all$Datetime <- unlist(lapply(all$Datetime, A))
all <- all %>% arrange(Datetime)
lagged <- all %>% lag()

awesome <- all 
awesome <- awesome %>% 
  mutate(diff = Datetime - (lagged %>% pull(Datetime)))

awesome %>% 
  select(diff) %>%
  max(na.rm = TRUE)
```

